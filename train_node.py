from argparse import ArgumentParser
import numpy as np

import torch

from utils import set_random_seed, split, load_config, create_optimizer
from evaluation import node_evaluation

from encoder import GraphEncoder
from autoencoder import GraphAutoEncoder
from torch_geometric.utils import (
    add_self_loops,
    remove_self_loops,
)
import scipy.sparse as sp
from collections import defaultdict
from itertools import combinations, chain
import torch
import torch.nn as nn
import torch.nn.functional as F
# import scipy.sparse as sp # åœ¨æ­¤ä»£ç æ®µä¸­æœªç›´æ¥ä½¿ç”¨
import numpy as np
from sklearn.decomposition import TruncatedSVD
import igraph as ig
from torch_geometric.datasets import Planetoid
from torch_geometric.utils import to_networkx
import tqdm
from triangle_motif_manager import TriangleMotifManager, TriangleAngleLoss, TriangleMotifLoss, sample_triangle_batch, TriangleDataset
from torch.utils.data import Dataset,DataLoader
import torch
import torch.multiprocessing as mp
from torch.utils.tensorboard import SummaryWriter
import os
from torch.cuda.amp import autocast, GradScaler
import gc


# 2. è®¡ç®—æ˜¾è‘—æ¨¡ä½“
def compute_significant_motifs(G, searchn=3, num_random=10, z_threshold=2.0):
    from collections import defaultdict
    import numpy as np
    print('searchn',searchn)
    # 1. å®šä¹‰å…³å¿ƒçš„ motif ID å’Œåç§°W
    if searchn == 3:
        # igraph: 0=ç©º, 1=å•è¾¹, 2=é“¾, 3=ä¸‰è§’å½¢
        motif_id_name = {
            2: 'U_Path',      # ä¸‰èŠ‚ç‚¹é“¾
            3: 'U_Triangle'   # ä¸‰è§’å½¢
        }
    elif searchn == 4:
        # 
        motif_id_name = {
            
            # 3æ¡è¾¹
            4: 'U4_Star',          # ID 4: æ˜Ÿå‹ (3æ¡è¾¹)
            5: 'U4_Path_P4',       # ID 5: é“¾å‹ (3æ¡è¾¹)
            
            # 4æ¡è¾¹
            7: 'U4_TailedTriangle',# ID 7: å¸¦å°¾ä¸‰è§’å½¢/çˆªå­å›¾ (4æ¡è¾¹)
            8: 'U4_Cycle_C4',      # ID 8: ç¯å‹/æ­£æ–¹å½¢ (4æ¡è¾¹)
            
            # 5æ¡è¾¹
            9: 'U4_Diamond',       # ID 9: è±å½¢å›¾ (5æ¡è¾¹)
            
            # 6æ¡è¾¹
            10: 'U4_Clique_K4'      # ID 10: å®Œå…¨å›¾ (6æ¡è¾¹)
        }
    else:
        raise ValueError("åªæ”¯æŒ3æˆ–4èŠ‚ç‚¹æ¨¡ä½“")

    # 2. çœŸå®å›¾ motif è®¡æ•°
    if searchn == 3:
        cut_prob = [0.5, 0.5, 0.5]
    elif searchn == 4:
        cut_prob = [0.5, 0.5, 0.5, 0.5]
    else:
        raise ValueError("åªæ”¯æŒ3æˆ–4èŠ‚ç‚¹æ¨¡ä½“")
    motif_counts_real = G.motifs_randesu(size=searchn,cut_prob=cut_prob)
    print('motif_counts_real',motif_counts_real)
    if motif_counts_real is None:
        print("é”™è¯¯: G.motifs_randesu ä¸ºçœŸå®å›¾è¿”å›äº† Noneã€‚æ— æ³•è®¡ç®— Z-scoresã€‚")
        return []

    # 3. éšæœºå›¾ motif è®¡æ•°
    random_counts_collection = defaultdict(list)
    degrees = G.degree()
    import tqdm
    print(f"ä¸º {num_random} ä¸ªéšæœºç½‘ç»œè®¡ç®—æ¨¡ä½“è®¡æ•° (æ— å‘)...")
    for i_random in tqdm.tqdm(range(num_random)):
        rand_G = ig.Graph.Degree_Sequence(degrees, method="configuration")
        # if searchn == 3:
        #     cut_prob = [0.01, 0.01, 0.01]
        # elif searchn == 4:
        #     cut_prob = [0.01, 0.01, 0.01, 0.01]
        current_rand_counts = rand_G.motifs_randesu(size=searchn,cut_prob=cut_prob)
        if current_rand_counts:
            for motif_id in motif_id_name:
                count = current_rand_counts[motif_id] if motif_id < len(current_rand_counts) else 0
                random_counts_collection[motif_id].append(count)

    # 4. è®¡ç®— Z-score
    significant_motifs = []
    for motif_id, motif_name in motif_id_name.items():
        real_count_val = motif_counts_real[motif_id] if motif_id < len(motif_counts_real) else 0
        if real_count_val == 0:
            continue
        counts_for_motif_in_random = random_counts_collection.get(motif_id, [])
        if not counts_for_motif_in_random:
            continue
        rand_mean = np.mean(counts_for_motif_in_random)
        rand_std = np.std(counts_for_motif_in_random)
        if rand_std > 0:
            z_score = (real_count_val - rand_mean) / rand_std
            if z_score > z_threshold:
                significant_motifs.append((motif_name, real_count_val, z_score, motif_id,searchn))
        elif real_count_val > rand_mean:
            significant_motifs.append((motif_name, real_count_val, float('inf'), motif_id,searchn))

    return significant_motifs
def load_igraph(data):
    edge_index = data.edge_index.cpu().numpy()
    num_nodes = data.num_nodes
    edges = list(zip(edge_index[0], edge_index[1]))
    G = ig.Graph(directed=False)
    G.add_vertices(num_nodes)
    G.add_edges(edges)
    G.simplify()
    return G
def compute_motif_link_matrix(G, target_motif_id_igraph,searchn=3):
    n = G.vcount()
    # ä½¿ç”¨å­—å…¸æ¥å­˜å‚¨è¾¹æƒé‡ï¼Œé¿å…é¢‘ç¹çš„çŸ©é˜µæ›´æ–°
    edge_weights = defaultdict(int)
    
    def _motif_link_callback(subgraph_obj, node_list_igraph, motif_id_found_igraph):
        if motif_id_found_igraph == target_motif_id_igraph:
            nodes = list(node_list_igraph)
            # ä½¿ç”¨itertools.combinationsæ¥ç”ŸæˆèŠ‚ç‚¹å¯¹
            for i, j in combinations(nodes, 2):
                edge_weights[(i, j)] += 1
                edge_weights[(j, i)] += 1
        return None 
    if searchn == 3:    
        cut_prob = [0.1, 0.1, 0.1]
    elif searchn == 4:
        cut_prob = [0.3, 0.3, 0.3, 0.3]
    else:
        raise ValueError("åªæ”¯æŒ3æˆ–4èŠ‚ç‚¹æ¨¡ä½“")
    # è®¡ç®—æ¨¡ä½“
    G.motifs_randesu(size=searchn,cut_prob = cut_prob, callback=_motif_link_callback)
    
    # ä¸€æ¬¡æ€§æ„å»ºç¨€ç–çŸ©é˜µ
    if edge_weights:
        rows, cols = zip(*edge_weights.keys())
        data = list(edge_weights.values())
        link_matrix = sp.csr_matrix((data, (rows, cols)), shape=(n, n))
    else:
        link_matrix = sp.csr_matrix((n, n))
    
    return link_matrix#min_max_normalize_sparse_matrix(link_matrix)
def build_motif_participation_matrix(G, motif_ids=[2,3,4,5,6, 7, 8, 9], searchn_list=[3,3,4,4,4, 4, 4, 4]):
    """
    G: igraph å›¾
    motif_ids: igraph ä¸­çš„ motif ç±»å‹ idï¼ˆå¦‚ä¸‰è§’å½¢æ˜¯3ï¼‰
    searchn_list: æ¯ä¸ª motif_id å¯¹åº”çš„ motif å¤§å°
    è¿”å›:
        M: numpy array (N, K)
    """
    N = G.vcount()
    # motif_nodetype_len = sum(searchn_list)
    # motif_nodetype_len_dic = {}
    # for i in range(len(motif_ids)):
    #     motif_nodetype_len_dic[motif_ids[i]] = sum(searchn_list[:i])
    M = np.zeros((N), dtype=np.float32)

    for motif_idx, (motif_id, size) in tqdm.tqdm(enumerate(zip(motif_ids, searchn_list)),total=len(motif_ids)):
        # igraphçš„motifs_randesuå‡½æ•°åœ¨è¾ƒæ–°ç‰ˆæœ¬ä¸­å¯èƒ½ä¸æ¥å—callbackï¼Œ
        # è¿™é‡Œçš„ç¤ºä¾‹ä»£ç éµå¾ªäº†æ—§ç‰ˆAPIçš„æ€æƒ³ã€‚
        # å¦‚æœä½¿ç”¨æ–°ç‰ˆigraphï¼Œå¯èƒ½éœ€è¦ç”¨motifs()åæ‰‹åŠ¨ç»Ÿè®¡ã€‚
        # æ­¤å¤„æˆ‘ä»¬å‡è®¾å®ƒèƒ½æŒ‰é¢„æœŸå·¥ä½œæˆ–ç”¨å…¶ä»–æ–¹å¼å¾—åˆ°äº†MçŸ©é˜µã€‚
        
        # ä¸ºäº†è®©ä»£ç å¯è¿è¡Œï¼Œæ­¤å¤„ä½¿ç”¨ä¸€ä¸ªç®€åŒ–çš„æ¨¡æ‹Ÿè®¡æ•°è¿‡ç¨‹
        # åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæ‚¨åº”è¯¥ä½¿ç”¨çœŸå®çš„motifè®¡æ•°å‡½æ•°
        try:
            # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–çš„ç¤ºä¾‹å›è°ƒï¼Œå®é™…çš„igraph APIå¯èƒ½æœ‰æ‰€ä¸åŒ
            def _callback(subgraph_obj, node_list_igraph, motif_id_found_igraph):
                if motif_id_found_igraph == motif_id:
                    # for v in node_list_igraph:
                    for index, v in enumerate(node_list_igraph):
                        M[v] += 1
                return None
            # cut_prob = [0.1] * size # cut_probæ˜¯å¯é€‰å‚æ•°ï¼Œç”¨äºè¿‘ä¼¼è®¡ç®—
            G.motifs_randesu(size=size, callback=_callback)#, cut_prob=cut_prob)
        except Exception as e:
            # å¦‚æœ `motifs_randesu` with callback ä¸å¯ç”¨ï¼Œæˆ‘ä»¬ç”¨ä¸€ä¸ªéšæœºæ–¹æ³•æ¨¡æ‹ŸM
            print(f"Warning: igraph motif counting failed with '{e}'. Using random M for demonstration.")
            # num_motifs_found = N * 2 # æ¨¡æ‹Ÿæ‰¾åˆ°çš„æ¨¡ä½“æ•°é‡
            # nodes_participating = np.random.randint(0, N, size=num_motifs_found)
            # for node_idx in nodes_participating:
            #     M[node_idx, motif_idx] += 1
                
    return M



def train_mae_epoch(graph_auto_encoder, x, edge_index, edge_index_pe, u, PE, optimizer, 
                   triangle_manager=None, triangle_angle_loss=None, triangle_motif_loss=None,triangle_counts_tensor=None,data_iterator=None,n_iter=None,argst=None,writer=None,scaler=None):
    graph_auto_encoder.train()
    triangles, negative_triplets = next(data_iterator)
    device = x.device # è·å–æ¨¡å‹/æ•°æ®çš„è®¾å¤‡
    triangles = triangles.to(device, non_blocking=True)
    # triangles = None
    # negative_triplets = negative_triplets.to(device, non_blocking=True)
    # ä¸»æŸå¤±ï¼ˆå¼€å¯æ··åˆç²¾åº¦ï¼Œå·²é…åˆ GradScaler ä½¿ç”¨ï¼‰
    # with autocast(enabled=True, dtype=torch.float16):
    main_loss,u_for_angle_loss = graph_auto_encoder(x, edge_index, u, PE, edge_index_pe, triangles=triangles)
    # u_for_angle_loss = graph_auto_encoder.encoder.embed(x, edge_index,PE)[0]
    # ä¸‰è§’å½¢æ¨¡ä½“æŸå¤±
    
    # # node_embeddings = graph_auto_encoder.U_hat
    # angle_loss = triangle_angle_loss(u_for_angle_loss, triangles,u)
    # motif_loss = triangle_motif_loss(u_for_angle_loss, triangles,negative_triplets)
    angle_loss = torch.tensor(0.0, device=main_loss.device)
    # # æ£€æŸ¥3: åç¦»åï¼Œangle_loss çš„åŸå§‹å€¼æ˜¯å¤šå°‘ï¼Ÿ
    # print(f"[Check 3] RAW angle_loss value: {angle_loss.item():.8f}")
    # if triangle_manager is not None and triangle_manager.get_triangle_count() > 0:
    #     # é‡‡æ ·ä¸‰è§’å½¢æ‰¹æ¬¡
    #     batch_size = 64
    #     triangles, negative_triplets, labels = sample_triangle_batch(triangle_manager, batch_size)
        
    #     if len(triangles) >0:
    #         # è·å–èŠ‚ç‚¹åµŒå…¥
    #         # with torch.no_grad():
    #         node_embeddings = graph_auto_encoder.U_hat#graph_auto_encoder.encoder.embed(x, edge_index, PE=PE)[1]
            
    #         # è§’åº¦é‡æ„æŸå¤±
    #         if triangle_angle_loss is not None:
    #             angle_loss = triangle_angle_loss(node_embeddings, triangles,u)
    #             triangle_loss += 0.1 * angle_loss
            
    #         # æ¨¡ä½“é¢„æµ‹æŸå¤±
    #         # if triangle_motif_loss is not None:
    #         #     motif_loss = triangle_motif_loss(node_embeddings, triangles, negative_triplets)
    #         #     triangle_loss += 0.1 * motif_loss
    #         motif_loss = torch.tensor(0.0, device=main_loss.device)
    motif_loss = torch.tensor(0.0, device=main_loss.device)#triangle_motif_loss(node_embeddings, triangles, negative_triplets)
    triangle_loss = torch.tensor(0.0, device=main_loss.device)
    # triangle_loss +=  0.1 * motif_loss
    # triangle_loss += argst.angle * angle_loss
    # æ€»æŸå¤±
    total_loss = main_loss #+ triangle_loss
    
    # æ£€æŸ¥lossæ˜¯å¦ä¸ºNaNï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡è¯¥stepï¼ˆåœ¨æ‰“å°å’Œè®°å½•ä¹‹å‰æ£€æŸ¥ï¼‰
    if torch.isnan(main_loss) or torch.isnan(total_loss):
        print(f"âš ï¸ Warning: NaN detected at epoch {n_iter}, skipping this step")
        return float('nan')
    
    writer.add_scalar('Loss/main_loss', main_loss.item(), n_iter)
    # writer.add_scalar('Loss/triangle_loss', triangle_loss.item(), n_iter)
    # writer.add_scalar('Loss/angle_loss', angle_loss.item(), n_iter)
    # writer.add_scalar('Loss/motif_loss', motif_loss.item(), n_iter)
    writer.add_scalar('Loss/total_loss', total_loss.item(), n_iter)
    
    print(f"main_loss: {main_loss.item():.4f}, total_loss: {total_loss.item():.4f}", end='\t')
    
    optimizer.zero_grad()
    scaler.scale(total_loss).backward()
    
    # æ¢¯åº¦è£å‰ªé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ - å¿…é¡»åœ¨backwardä¹‹åã€stepä¹‹å‰æ‰§è¡Œ
    scaler.unscale_(optimizer)  # åœ¨æ··åˆç²¾åº¦è®­ç»ƒä¸­ï¼Œéœ€è¦å…ˆunscaleæ‰èƒ½è£å‰ªæ¢¯åº¦
    torch.nn.utils.clip_grad_norm_(graph_auto_encoder.parameters(), max_norm=1.0)
    
    # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦åŒ…å«NaN
    has_nan_grad = False
    for param in graph_auto_encoder.parameters():
        if param.grad is not None and torch.isnan(param.grad).any():
            has_nan_grad = True
            break
    
    if has_nan_grad:
        print(f"âš ï¸ Warning: NaN gradient detected at epoch {n_iter}, skipping this step")
        scaler.update()  # å³ä½¿è·³è¿‡stepï¼Œä¹Ÿè¦æ›´æ–°scaler
        return float('nan')
    
    scaler.step(optimizer)
    scaler.update()
    
    
    return total_loss.item()  # è¿”å›losså€¼ä¾›å¤–éƒ¨ç›‘æ§
import random
def worker_init_fn(worker_id):
    """
    ä¸ºæ¯ä¸ª DataLoader worker è®¾ç½®ç‹¬ç«‹çš„éšæœºç§å­ã€‚
    è¿™å¯¹äºå¯å¤ç°çš„æ•°æ®åŠ è½½å’Œå¢å¼ºè‡³å…³é‡è¦ã€‚
    """
    # è·å–åœ¨ä¸»è¿›ç¨‹ä¸­è®¾ç½®çš„å…¨å±€ç§å­
    # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¸èƒ½ç›´æ¥ç”¨ ep_numï¼Œå› ä¸ºè¿™ä¸ªå‡½æ•°æ˜¯åœ¨ DataLoader å†…éƒ¨è°ƒç”¨çš„
    # ä¸€ä¸ªå¸¸ç”¨çš„æ–¹æ³•æ˜¯ä½¿ç”¨ torch.initial_seed()ï¼Œå®ƒä¼šè¿”å›ä¸»è¿›ç¨‹ä¸ºå½“å‰ worker è®¾ç½®çš„åˆå§‹ç§å­
    worker_seed = torch.initial_seed() % 2**32
    np.random.seed(worker_seed)
    random.seed(worker_seed)
    torch.manual_seed(worker_seed) # <--- æ·»åŠ è¿™ä¸€è¡Œ

if __name__ == '__main__':

    # å¿…é¡»å°†è¿™è¡Œä»£ç æ”¾åœ¨ __main__ çš„æœ€å¼€å§‹ï¼
    try:
        mp.set_start_method('spawn', force=True)
        print("Spawn Cuda Process")
    except RuntimeError:
        pass
    
    parser = ArgumentParser()
    parser.add_argument('--num_exp', type=int, default=1)
    parser.add_argument('--root', type=str, default="./dataset")
    parser.add_argument('--dataset', type=str, default="cora")  # [pubmed citeseer minesweeper"blog", "chameleon", "squirrel", "actor","cornell","wisconsin","deezereurope","flickr","texas"]
    args = parser.parse_args()
    
    config = load_config(f"./config/{args.dataset}.yaml")
    for key, value in config.items():
        setattr(args, key, value)
    args.device = torch.device("cuda:" + str(args.device)) if torch.cuda.is_available() else torch.device("cpu")
    
    # for motif_num in [1024 ,2048,4096,8192]: #1024 ,2048,4096,
        
    #     args.epochs = 1200
    #     if args.dataset == "chameleon":
    #         args.angle = 0.1
    #         args.epochs = 1000
    #     else:
    #         args.angle = 0.01
    #     if args.dataset == "actor":
    #         args.angle = 1.0
    #         args.epochs = 1200
    #     elif args.dataset == "citeseer":#or args.dataset == "pubmed":
    #         args.epochs = 500
    #     elif args.dataset == "cora":
    #         args.epochs = 300
    #     elif args.dataset == "texas":
    #         args.epochs = 800
    #     elif args.dataset == "citeseer":
    #         args.epochs = 1000
    #         # args.masked_pe_loss= 0.001
    #     # if args.dataset == "cornell":
    #     #     args.epochs = 1000
    #     #     args.angle = 1.0
    #     # args.angle = 0.01

    #     print(args)
        
    #     data = torch.load('../dataset/{}.pt'.format(args.dataset))
    #     print(data)
    #     x = data.x.float().to(args.device)
    #     if x.shape[1] > 4096:
    #         svd = TruncatedSVD(n_components=4096, random_state=0)
    #         x_cpu = x.detach().cpu().numpy()
    #         x_reduced = svd.fit_transform(x_cpu)
    #         x = torch.from_numpy(x_reduced).float().to(args.device)
    #         args.feat_dim = x.shape[1]
        
    #     # åœ¨åˆ›å»ºæ¨¡å‹ä¹‹å‰æ·»åŠ 
    #     print('å®é™…çš„ç‰¹å¾ç»´åº¦')
    #     print(x.shape)
    #     args.feat_dim = x.shape[1]  # ä½¿ç”¨å®é™…æ•°æ®çš„ç‰¹å¾ç»´åº¦
    #     args.num_node = x.shape[0]
    #     edge = data.edge_index.long().to(args.device)
    #     e = data.e[:args.max_freqs].float().to(args.device)
    #     u = data.u[:, :args.max_freqs].float().to(args.device)

    #     y = data.y.to(args.device)
    #     print(y.min().item(), y.max().item())
    #     nclass = y.max().item() + 1

    #     edge_index_pe, _ = remove_self_loops(edge, None)
    #     edge_index_pe, _ = add_self_loops(edge_index_pe, fill_value='mean', num_nodes=u.shape[0])
    #     PE = torch.linalg.norm(u[edge_index_pe[0]] - u[edge_index_pe[1]], dim=-1)  # [e_sum, 1]

    #     # åœ¨æ•°æ®åŠ è½½åæ·»åŠ ä¸‰è§’å½¢æ¨¡ä½“ç®¡ç†å™¨åˆå§‹åŒ–
    #     print("ğŸ” åˆå§‹åŒ–ä¸‰è§’å½¢æ¨¡ä½“ç®¡ç†å™¨...")
    #     triangle_manager = TriangleMotifManager(data.edge_index.long(), x.shape[0], args.device)
    #     print(f"âœ… æ‰¾åˆ° {triangle_manager.get_triangle_count()} ä¸ªä¸‰è§’å½¢")
    #     # ### ===========================å°è¯•ç”¨æ–°çš„æ€è·¯===========================
    #     # edge_index_np = edge.cpu().numpy()
    #     # edges = list(zip(edge_index_np[0], edge_index_np[1]))

    #     # G = ig.Graph(directed=False)
    #     # G.add_vertices(x.shape[0])
    #     # G.add_edges(edges)
    #     # G.simplify()
    #     # # 3. è°ƒç”¨æ ¸å¿ƒå‡½æ•° count_triangles()
    #     # # è¿™ä¸ªå‡½æ•°è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨çš„é•¿åº¦æ˜¯èŠ‚ç‚¹æ•°ï¼Œå€¼æ˜¯æ¯ä¸ªèŠ‚ç‚¹å‚ä¸çš„ä¸‰è§’å½¢æ•°
    #     # triangle_counts_list = build_motif_participation_matrix(G,[3],[3])
        
    #     # # 4. å°†ç»“æœè½¬æ¢ä¸º PyTorch Tensor
    #     # triangle_counts_tensor = torch.tensor(triangle_counts_list, dtype=torch.float)
        
    #     # if args.dataset == "actor":
    #     #     batch_size = 7121
    #     # elif args.dataset == "cornell":
    #     #     batch_size = 59
    #     if triangle_manager.get_triangle_count() < motif_num:
    #         batch_size = triangle_manager.get_triangle_count()
    #     else:
    #         batch_size = motif_num
        
        
    #     import time
    #     import datetime
    #     nowtime_step = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    #     time1 = time.time()
    #     args.num_exp = 10
    #     final_results = []
    #     for ep_num in range(0,args.num_exp):
    #         scaler = GradScaler() # åœ¨è®­ç»ƒå¾ªç¯å¤–åˆå§‹åŒ–
    #         path_write = './logdata/{}{}{}{}exp{}encoderUhat11'.format(nowtime_step,args.dataset,args.epochs,args.angle,ep_num)
    #         os.makedirs(path_write, exist_ok=True)
    #         writer = SummaryWriter(path_write)
    #         # 1. åˆ›å»ºè¿­ä»£å™¨å¯¹è±¡
            
    #         args.seed = ep_num
    #         set_random_seed(ep_num)
    #         dataset = TriangleDataset(triangle_manager, epoch_size=args.epochs+1, batch_size=batch_size, negative_ratio=2.0)
    #         if args.dataset == "flickr":
    #             dataloader = DataLoader(
    #                 dataset,
    #                 batch_size=None,  # å› ä¸ºæˆ‘ä»¬çš„ __getitem__ å·²ç»è¿”å›äº†æ‰¹æ¬¡
    #                 num_workers=1,    # ä½¿ç”¨4ä¸ªå­è¿›ç¨‹åœ¨åå°åŠ è½½æ•°æ®ï¼Œè¿™ä¸ªå€¼å¯ä»¥æ ¹æ®ä½ çš„CPUæ ¸æ•°è°ƒæ•´
    #                 pin_memory=True,   # å¦‚æœä½¿ç”¨GPUï¼Œå¯ä»¥åŠ é€Ÿæ•°æ®ä»CPUåˆ°GPUçš„ä¼ è¾“
    #                 worker_init_fn=worker_init_fn,
    #                 # persistent_workers=True, # å»ºè®®ä¿ç•™è¿™äº›ä¼˜åŒ–å‚æ•°
    #                 # prefetch_factor=4
    #             )
    #         else:
    #             dataloader = DataLoader(
    #                 dataset,
    #                 batch_size=None,  # å› ä¸ºæˆ‘ä»¬çš„ __getitem__ å·²ç»è¿”å›äº†æ‰¹æ¬¡
    #                 num_workers=8,    # ä½¿ç”¨4ä¸ªå­è¿›ç¨‹åœ¨åå°åŠ è½½æ•°æ®ï¼Œè¿™ä¸ªå€¼å¯ä»¥æ ¹æ®ä½ çš„CPUæ ¸æ•°è°ƒæ•´
    #                 pin_memory=True,   # å¦‚æœä½¿ç”¨GPUï¼Œå¯ä»¥åŠ é€Ÿæ•°æ®ä»CPUåˆ°GPUçš„ä¼ è¾“
    #                 worker_init_fn=worker_init_fn,
    #                 # persistent_workers=True, # å»ºè®®ä¿ç•™è¿™äº›ä¼˜åŒ–å‚æ•°
    #                 # prefetch_factor=4
    #             )
    #         data_iterator = iter(dataloader)
    #         print('Checking data attributes')
    #         if hasattr(data, 'train_mask'):
    #             if len(data.train_mask.size()) > 1:
    #                 train_idx = torch.where(data.train_mask[:, args.seed])[0]
    #                 val_idx = torch.where(data.val_mask[:, args.seed])[0]
    #                 test_idx = torch.where(data.test_mask[:, args.seed])[0]
    #             else:
    #                 train_idx = torch.where(data.train_mask)[0]
    #                 val_idx = torch.where(data.val_mask)[0]
    #                 test_idx = torch.where(data.test_mask)[0]
    #         else:
    #             train_idx, val_idx, test_idx = split(y)

    #         # åˆ›å»ºä¸‰è§’å½¢ç›¸å…³çš„æŸå¤±å‡½æ•°
    #         triangle_angle_loss = TriangleAngleLoss().to(args.device)
    #         triangle_motif_loss = TriangleMotifLoss(u.shape[1]).to(args.device)
    #         encoder = GraphEncoder(out_dim=args.embed_dim, args=args).to(args.device)
    #         model = GraphAutoEncoder(encoder=encoder, num_atom_type=args.feat_dim, args=args).to(args.device)

    #         parameters = model.parameters()#chain(model.parameters(), triangle_motif_loss.parameters())
    #         # --- è¯·ä¿®æ”¹æˆä¸‹é¢è¿™æ · ---

    #         # 1. é¦–å…ˆï¼ŒæŠŠæ¨¡å‹çš„å‚æ•°åˆ†æˆä¸¤ç»„

    #         # ç¬¬1ç»„ï¼šåªåŒ…å« U_hatã€‚æˆ‘ä»¬ä¸ºå®ƒä¸“é—¨æŒ‡å®šä¸€ä¸ªé«˜å­¦ä¹ ç‡ã€‚
    #         # è¿™é‡Œçš„ '* 100' æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™ä¸ªç³»æ•° (æ¯”å¦‚ 50, 200)ã€‚
    #         # u_hat_param_group = {
    #         #     'params': model.U_hat, 
    #         #     'lr': args.init_lr * 100  
    #         # }

    #         # # ç¬¬2ç»„ï¼šåŒ…å«æ¨¡å‹ä¸­é™¤äº† U_hat ä»¥å¤–çš„æ‰€æœ‰å…¶ä»–å‚æ•°ã€‚
    #         # # æˆ‘ä»¬ä¸åœ¨è¿™é‡Œä¸ºå®ƒæŒ‡å®š 'lr'ï¼Œè¿™æ ·å®ƒå°±ä¼šè‡ªåŠ¨ä½¿ç”¨å‡½æ•°è°ƒç”¨æ—¶çš„é»˜è®¤å­¦ä¹ ç‡ã€‚
    #         # other_params_group = {
    #         #     'params': [p for n, p in model.named_parameters() if 'U_hat' not in n]
    #         # }

    #         # # 2. å°†è¿™ä¸¤ç»„å‚æ•°æ‰“åŒ…æˆä¸€ä¸ªåˆ—è¡¨
    #         # # è¿™å°±æ˜¯æˆ‘ä»¬è¦ä¼ é€’ç»™ä¼˜åŒ–å™¨çš„æ–° `parameters`
    #         # parameters = [u_hat_param_group, other_params_group]

    #         if args.optim == "sgd":
    #             pass
    #         else:
    #             args.momentum = None
    #         optimizer = create_optimizer(opt=args.optim, parameters=parameters, lr=args.init_lr, weight_decay=float(args.weight_decay), momentum=args.momentum)

    #         if args.use_schedule:
    #             scheduler = lambda epoch :( 1 + np.cos((epoch) * np.pi / args.epochs) ) * 0.5
    #             scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)
    #         else:
    #             scheduler = None

    #         for epoch in range(1, args.epochs+1):
    #             train_mae_epoch(graph_auto_encoder=model, x=x, edge_index=edge, u=u, PE=PE, edge_index_pe=edge_index_pe, optimizer=optimizer, 
    #                     triangle_manager=triangle_manager, triangle_angle_loss=triangle_angle_loss, triangle_motif_loss=triangle_motif_loss,data_iterator=data_iterator,n_iter=epoch,argst=args,writer=writer,scaler=scaler)
    #             print('epoch: ',epoch)
    #             # åœ¨æ¯ä¸ªepochç»“æŸåæ·»åŠ 
    #             # if epoch % 100 == 0:  # æ¯100ä¸ªepochæ£€æŸ¥ä¸€æ¬¡
    #             #     with torch.no_grad():
    #             #         u_hat_norm = torch.norm(model.U_hat, dim=1).mean().item()
    #             #         u_hat_deviation = torch.mean((model.U_hat - torch.eye(model.U_hat.size(0), device=model.U_hat.device)).abs()).item()
    #             #         print(f"[Epoch {epoch}] U_hat norm: {u_hat_norm:.4f}, deviation: {u_hat_deviation:.6f}")
    #             if scheduler:
    #                 scheduler.step()
    #         time2 = time.time()
    #         print('consume time {}'.format(time2-time1))
    #         # print(model.weight_motif)
    #         model.eval()
    #         triangles, negative_triplets = next(data_iterator)
    #         device = x.device # è·å–æ¨¡å‹/æ•°æ®çš„è®¾å¤‡
    #         triangles = triangles.to(device, non_blocking=True)
    #         # triangles = None
    #         with torch.no_grad():
    #             embed = model.embed(x, edge, u, edge_index_pe, triangles=triangles)
    #         acc, pred = node_evaluation(emb=embed, y=y, train_idx=train_idx, valid_idx=val_idx, test_idx=test_idx, epochs=args.epochs_eval, lr=args.lr_eval, weight_decay=args.wd_eval)
    #         print(f"Epoch {epoch}, ACC: {acc.item()}")
    #         writer.add_scalar('EXP/ACC', acc.item(), ep_num)
    #         final_results.append(acc.item())
            
    #         # ä¿å­˜æ¨¡å‹checkpointï¼ˆç”¨äºåç»­å¯è§†åŒ–ï¼‰
    #         # checkpoint_path = os.path.join(path_write, f'model_checkpoint_exp{ep_num}.pt')
    #         # torch.save({
    #         #     'model_state_dict': model.state_dict(),
    #         #     'encoder_state_dict': encoder.state_dict(),
    #         #     'acc': acc.item(),
    #         #     'epoch': epoch,
    #         #     'args': args,
    #         # }, checkpoint_path)
    #         # print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {checkpoint_path}")

    #         # ==== èµ„æºæ¸…ç†ï¼Œé¿å…å¤šæ¬¡å®éªŒç´¯ç§¯æ˜¾å­˜å ç”¨ ====
    #         try:
    #             writer.close()
    #         except Exception:
    #             pass
    #         # åˆ é™¤è¿­ä»£å™¨/æ•°æ®åŠ è½½å™¨/æ•°æ®é›†
    #         try:
    #             del data_iterator
    #         except Exception:
    #             pass
    #         try:
    #             del dataloader
    #         except Exception:
    #             pass
    #         try:
    #             del dataset
    #         except Exception:
    #             pass
    #         # åˆ é™¤æ¨¡å‹ä¸ä¼˜åŒ–ç›¸å…³å¯¹è±¡
    #         try:
    #             del model
    #         except Exception:
    #             pass
    #         try:
    #             del encoder
    #         except Exception:
    #             pass
    #         try:
    #             del optimizer
    #         except Exception:
    #             pass
    #         try:
    #             del scheduler
    #         except Exception:
    #             pass
    #         try:
    #             del triangles
    #         except Exception:
    #             pass
    #         try:
    #             del triangle_motif_loss
    #         except Exception:
    #             pass
    #         try:
    #             del scaler
    #         except Exception:
    #             pass
    #         # å¼ºåˆ¶å›æ”¶å¹¶æ¸…ç©º CUDA ç¼“å­˜
    #         gc.collect()
    #         if torch.cuda.is_available():
    #             torch.cuda.empty_cache()

    #     mean_final_result = np.mean(final_results)
    #     std_final_result = np.std(final_results)
    #     print(f"{final_results}")
    #     print(f"final result: {mean_final_result:.5f}Â±{std_final_result:.5}")
    #     print(f"final result: {mean_final_result*100:.2f}Â±{std_final_result*100:.2f}")
    for pe_loss_lameda in [args.masked_pe_loss]:# #[0.001, 0.005, 0.01, 0.05, 0.1, 0.5, 1.0]
        args.masked_pe_loss = pe_loss_lameda
        motif_num = 8192
        args.epochs = 1200
        if args.dataset == "chameleon":
            args.angle = 0.1
            args.epochs = 1000
        else:
            args.angle = 0.01
        if args.dataset == "actor":
            args.angle = 1.0
            args.epochs = 1200
        elif args.dataset == "citeseer":#or args.dataset == "pubmed":
            args.epochs = 500
        elif args.dataset == "cora":
            args.epochs = 300
        elif args.dataset == "texas":
            args.epochs = 800
        elif args.dataset == "citeseer":
            args.epochs = 1000
            # args.masked_pe_loss= 0.001
        # if args.dataset == "cornell":
        #     args.epochs = 1000
        #     args.angle = 1.0
        # args.angle = 0.01

        print(args)
        
        data = torch.load('../dataset/{}.pt'.format(args.dataset))
        print(data)
        x = data.x.float().to(args.device)
        if x.shape[1] > 4096:
            svd = TruncatedSVD(n_components=4096, random_state=0)
            x_cpu = x.detach().cpu().numpy()
            x_reduced = svd.fit_transform(x_cpu)
            x = torch.from_numpy(x_reduced).float().to(args.device)
            args.feat_dim = x.shape[1]
        
        # åœ¨åˆ›å»ºæ¨¡å‹ä¹‹å‰æ·»åŠ 
        print('å®é™…çš„ç‰¹å¾ç»´åº¦')
        print(x.shape)
        args.feat_dim = x.shape[1]  # ä½¿ç”¨å®é™…æ•°æ®çš„ç‰¹å¾ç»´åº¦
        args.num_node = x.shape[0]
        edge = data.edge_index.long().to(args.device)
        e = data.e[:args.max_freqs].float().to(args.device)
        u = data.u[:, :args.max_freqs].float().to(args.device)

        y = data.y.to(args.device)
        print(y.min().item(), y.max().item())
        nclass = y.max().item() + 1

        edge_index_pe, _ = remove_self_loops(edge, None)
        edge_index_pe, _ = add_self_loops(edge_index_pe, fill_value='mean', num_nodes=u.shape[0])
        PE = torch.linalg.norm(u[edge_index_pe[0]] - u[edge_index_pe[1]], dim=-1)  # [e_sum, 1]

        # åœ¨æ•°æ®åŠ è½½åæ·»åŠ ä¸‰è§’å½¢æ¨¡ä½“ç®¡ç†å™¨åˆå§‹åŒ–
        print("ğŸ” åˆå§‹åŒ–ä¸‰è§’å½¢æ¨¡ä½“ç®¡ç†å™¨...")
        triangle_manager = TriangleMotifManager(data.edge_index.long(), x.shape[0], args.device)
        print(f"âœ… æ‰¾åˆ° {triangle_manager.get_triangle_count()} ä¸ªä¸‰è§’å½¢")
        # ### ===========================å°è¯•ç”¨æ–°çš„æ€è·¯===========================
        # edge_index_np = edge.cpu().numpy()
        # edges = list(zip(edge_index_np[0], edge_index_np[1]))

        # G = ig.Graph(directed=False)
        # G.add_vertices(x.shape[0])
        # G.add_edges(edges)
        # G.simplify()
        # # 3. è°ƒç”¨æ ¸å¿ƒå‡½æ•° count_triangles()
        # # è¿™ä¸ªå‡½æ•°è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨çš„é•¿åº¦æ˜¯èŠ‚ç‚¹æ•°ï¼Œå€¼æ˜¯æ¯ä¸ªèŠ‚ç‚¹å‚ä¸çš„ä¸‰è§’å½¢æ•°
        # triangle_counts_list = build_motif_participation_matrix(G,[3],[3])
        
        # # 4. å°†ç»“æœè½¬æ¢ä¸º PyTorch Tensor
        # triangle_counts_tensor = torch.tensor(triangle_counts_list, dtype=torch.float)
        
        # if args.dataset == "actor":
        #     batch_size = 7121
        # elif args.dataset == "cornell":
        #     batch_size = 59
        if triangle_manager.get_triangle_count() < motif_num:
            batch_size = triangle_manager.get_triangle_count()
        else:
            batch_size = motif_num
        
        
        import time
        import datetime
        nowtime_step = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        time1 = time.time()
        args.num_exp = 10
        final_results = []
        for ep_num in range(0,args.num_exp):
            scaler = GradScaler() # åœ¨è®­ç»ƒå¾ªç¯å¤–åˆå§‹åŒ–
            path_write = './logdata/{}{}{}{}exp{}encodermotifself'.format(nowtime_step,args.dataset,args.epochs,args.angle,ep_num)
            os.makedirs(path_write, exist_ok=True)
            writer = SummaryWriter(path_write)
            # 1. åˆ›å»ºè¿­ä»£å™¨å¯¹è±¡
            
            args.seed = ep_num
            set_random_seed(ep_num)
            dataset = TriangleDataset(triangle_manager, epoch_size=args.epochs+1, batch_size=batch_size, negative_ratio=2.0)
            if args.dataset == "flickr":
                dataloader = DataLoader(
                    dataset,
                    batch_size=None,  # å› ä¸ºæˆ‘ä»¬çš„ __getitem__ å·²ç»è¿”å›äº†æ‰¹æ¬¡
                    num_workers=1,    # ä½¿ç”¨4ä¸ªå­è¿›ç¨‹åœ¨åå°åŠ è½½æ•°æ®ï¼Œè¿™ä¸ªå€¼å¯ä»¥æ ¹æ®ä½ çš„CPUæ ¸æ•°è°ƒæ•´
                    pin_memory=True,   # å¦‚æœä½¿ç”¨GPUï¼Œå¯ä»¥åŠ é€Ÿæ•°æ®ä»CPUåˆ°GPUçš„ä¼ è¾“
                    worker_init_fn=worker_init_fn,
                    # persistent_workers=True, # å»ºè®®ä¿ç•™è¿™äº›ä¼˜åŒ–å‚æ•°
                    # prefetch_factor=4
                )
            else:
                dataloader = DataLoader(
                    dataset,
                    batch_size=None,  # å› ä¸ºæˆ‘ä»¬çš„ __getitem__ å·²ç»è¿”å›äº†æ‰¹æ¬¡
                    num_workers=8,    # ä½¿ç”¨4ä¸ªå­è¿›ç¨‹åœ¨åå°åŠ è½½æ•°æ®ï¼Œè¿™ä¸ªå€¼å¯ä»¥æ ¹æ®ä½ çš„CPUæ ¸æ•°è°ƒæ•´
                    pin_memory=True,   # å¦‚æœä½¿ç”¨GPUï¼Œå¯ä»¥åŠ é€Ÿæ•°æ®ä»CPUåˆ°GPUçš„ä¼ è¾“
                    worker_init_fn=worker_init_fn,
                    # persistent_workers=True, # å»ºè®®ä¿ç•™è¿™äº›ä¼˜åŒ–å‚æ•°
                    # prefetch_factor=4
                )
            data_iterator = iter(dataloader)
            print('Checking data attributes')
            if hasattr(data, 'train_mask'):
                if len(data.train_mask.size()) > 1:
                    train_idx = torch.where(data.train_mask[:, args.seed])[0]
                    val_idx = torch.where(data.val_mask[:, args.seed])[0]
                    test_idx = torch.where(data.test_mask[:, args.seed])[0]
                else:
                    train_idx = torch.where(data.train_mask)[0]
                    val_idx = torch.where(data.val_mask)[0]
                    test_idx = torch.where(data.test_mask)[0]
            else:
                train_idx, val_idx, test_idx = split(y)

            # åˆ›å»ºä¸‰è§’å½¢ç›¸å…³çš„æŸå¤±å‡½æ•°
            triangle_angle_loss = TriangleAngleLoss().to(args.device)
            triangle_motif_loss = TriangleMotifLoss(u.shape[1]).to(args.device)
            encoder = GraphEncoder(out_dim=args.embed_dim, args=args).to(args.device)
            model = GraphAutoEncoder(encoder=encoder, num_atom_type=args.feat_dim, args=args).to(args.device)

            parameters = model.parameters()#chain(model.parameters(), triangle_motif_loss.parameters())
            # --- è¯·ä¿®æ”¹æˆä¸‹é¢è¿™æ · ---

            # 1. é¦–å…ˆï¼ŒæŠŠæ¨¡å‹çš„å‚æ•°åˆ†æˆä¸¤ç»„

            # ç¬¬1ç»„ï¼šåªåŒ…å« U_hatã€‚æˆ‘ä»¬ä¸ºå®ƒä¸“é—¨æŒ‡å®šä¸€ä¸ªé«˜å­¦ä¹ ç‡ã€‚
            # è¿™é‡Œçš„ '* 100' æ˜¯ä¸€ä¸ªä¾‹å­ï¼Œæ‚¨å¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´è¿™ä¸ªç³»æ•° (æ¯”å¦‚ 50, 200)ã€‚
            # u_hat_param_group = {
            #     'params': model.U_hat, 
            #     'lr': args.init_lr * 100  
            # }

            # # ç¬¬2ç»„ï¼šåŒ…å«æ¨¡å‹ä¸­é™¤äº† U_hat ä»¥å¤–çš„æ‰€æœ‰å…¶ä»–å‚æ•°ã€‚
            # # æˆ‘ä»¬ä¸åœ¨è¿™é‡Œä¸ºå®ƒæŒ‡å®š 'lr'ï¼Œè¿™æ ·å®ƒå°±ä¼šè‡ªåŠ¨ä½¿ç”¨å‡½æ•°è°ƒç”¨æ—¶çš„é»˜è®¤å­¦ä¹ ç‡ã€‚
            # other_params_group = {
            #     'params': [p for n, p in model.named_parameters() if 'U_hat' not in n]
            # }

            # # 2. å°†è¿™ä¸¤ç»„å‚æ•°æ‰“åŒ…æˆä¸€ä¸ªåˆ—è¡¨
            # # è¿™å°±æ˜¯æˆ‘ä»¬è¦ä¼ é€’ç»™ä¼˜åŒ–å™¨çš„æ–° `parameters`
            # parameters = [u_hat_param_group, other_params_group]

            if args.optim == "sgd":
                pass
            else:
                args.momentum = None
            optimizer = create_optimizer(opt=args.optim, parameters=parameters, lr=args.init_lr, weight_decay=float(args.weight_decay), momentum=args.momentum)

            if args.use_schedule:
                scheduler = lambda epoch :( 1 + np.cos((epoch) * np.pi / args.epochs) ) * 0.5
                scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)
            else:
                scheduler = None

            for epoch in range(1, args.epochs+1):
                train_mae_epoch(graph_auto_encoder=model, x=x, edge_index=edge, u=u, PE=PE, edge_index_pe=edge_index_pe, optimizer=optimizer, 
                        triangle_manager=triangle_manager, triangle_angle_loss=triangle_angle_loss, triangle_motif_loss=triangle_motif_loss,data_iterator=data_iterator,n_iter=epoch,argst=args,writer=writer,scaler=scaler)
                print('epoch: ',epoch)
                # åœ¨æ¯ä¸ªepochç»“æŸåæ·»åŠ 
                # if epoch % 100 == 0:  # æ¯100ä¸ªepochæ£€æŸ¥ä¸€æ¬¡
                #     with torch.no_grad():
                #         u_hat_norm = torch.norm(model.U_hat, dim=1).mean().item()
                #         u_hat_deviation = torch.mean((model.U_hat - torch.eye(model.U_hat.size(0), device=model.U_hat.device)).abs()).item()
                #         print(f"[Epoch {epoch}] U_hat norm: {u_hat_norm:.4f}, deviation: {u_hat_deviation:.6f}")
                if scheduler:
                    scheduler.step()
            time2 = time.time()
            print('consume time {}'.format(time2-time1))
            # print(model.weight_motif)
            model.eval()
            triangles, negative_triplets = next(data_iterator)
            device = x.device # è·å–æ¨¡å‹/æ•°æ®çš„è®¾å¤‡
            triangles = triangles.to(device, non_blocking=True)
            # triangles = None
            with torch.no_grad():
                embed = model.embed(x, edge, u, edge_index_pe, triangles=triangles)
            acc, pred = node_evaluation(emb=embed, y=y, train_idx=train_idx, valid_idx=val_idx, test_idx=test_idx, epochs=args.epochs_eval, lr=args.lr_eval, weight_decay=args.wd_eval)
            print(f"Epoch {epoch}, ACC: {acc.item()}")
            writer.add_scalar('EXP/ACC', acc.item(), ep_num)
            final_results.append(acc.item())
            
            # ä¿å­˜æ¨¡å‹checkpointï¼ˆç”¨äºåç»­å¯è§†åŒ–ï¼‰
            checkpoint_path = os.path.join(path_write, f'model_checkpoint_exp{ep_num}.pt')
            torch.save({
                'model_state_dict': model.state_dict(),
                'encoder_state_dict': encoder.state_dict(),
                'acc': acc.item(),
                'epoch': epoch,
                'args': args,
            }, checkpoint_path)
            print(f"æ¨¡å‹å·²ä¿å­˜è‡³: {checkpoint_path}")

            # ==== èµ„æºæ¸…ç†ï¼Œé¿å…å¤šæ¬¡å®éªŒç´¯ç§¯æ˜¾å­˜å ç”¨ ====
            try:
                writer.close()
            except Exception:
                pass
            # åˆ é™¤è¿­ä»£å™¨/æ•°æ®åŠ è½½å™¨/æ•°æ®é›†
            try:
                del data_iterator
            except Exception:
                pass
            try:
                del dataloader
            except Exception:
                pass
            try:
                del dataset
            except Exception:
                pass
            # åˆ é™¤æ¨¡å‹ä¸ä¼˜åŒ–ç›¸å…³å¯¹è±¡
            try:
                del model
            except Exception:
                pass
            try:
                del encoder
            except Exception:
                pass
            try:
                del optimizer
            except Exception:
                pass
            try:
                del scheduler
            except Exception:
                pass
            try:
                del triangle_angle_loss
            except Exception:
                pass
            try:
                del triangle_motif_loss
            except Exception:
                pass
            try:
                del scaler
            except Exception:
                pass
            # å¼ºåˆ¶å›æ”¶å¹¶æ¸…ç©º CUDA ç¼“å­˜
            gc.collect()
            if torch.cuda.is_available():
                torch.cuda.empty_cache()

        mean_final_result = np.mean(final_results)
        std_final_result = np.std(final_results)
        print(f"{final_results}")
        print(f"pe_loss_lameda: {pe_loss_lameda}")
        print(f"final result: {mean_final_result:.5f}Â±{std_final_result:.5}")
        print(f"final result: {mean_final_result*100:.2f}Â±{std_final_result*100:.2f}")